#!/usr/bin/env python3
"""
X Profile Analyzer - ç”¨æˆ·ç”»åƒåˆ†æå·¥å…·
é€šè¿‡ Nitter (via Camofox) æŠ“å–æ¨æ–‡ï¼Œç”Ÿæˆç»“æ„åŒ–ç”¨æˆ·ç”»åƒ

Usage:
    # å®Œæ•´æ¨¡å¼ï¼ˆæŠ“æ¨æ–‡ + AI åˆ†æï¼‰
    python3 x_profile_analyzer.py --user elonmusk

    # çº¯æ•°æ®æ¨¡å¼ï¼ˆåªæŠ“æ¨æ–‡ï¼Œä¸è°ƒ AIï¼‰è®©é¾™è™¾è‡ªå·±åˆ†æ
    python3 x_profile_analyzer.py --user elonmusk --no-analyze

    # æ•°æ®æ¨¡å¼ + ä¿å­˜åŸå§‹ JSON
    python3 x_profile_analyzer.py --user elonmusk --no-analyze --output-json data.json

AI é…ç½®ï¼ˆå®Œæ•´æ¨¡å¼éœ€è¦ï¼Œä¸‰é€‰ä¸€ï¼‰ï¼š
    export MINIMAX_API_KEY=xxx     # MiniMaxï¼ˆOpenClaw ç”¨æˆ·è‡ªåŠ¨è¯»å–ï¼Œæ— éœ€é…ç½®ï¼‰
    export OPENAI_API_KEY=xxx      # OpenAI / DeepSeek ç­‰
    export OPENAI_BASE_URL=xxx     # è‡ªå®šä¹‰æ¥å£ï¼ˆå¯é€‰ï¼‰
    export OPENAI_MODEL=xxx        # æ¨¡å‹åï¼ˆå¯é€‰ï¼Œé»˜è®¤ gpt-4o-miniï¼‰
"""

import json
import re
import sys
import os
import time
import argparse
import urllib.request
import urllib.error
from datetime import datetime
from typing import Optional, Dict, List, Any, Tuple
from pathlib import Path


# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CAMOFOX_PORT = 9377
NITTER_INSTANCE = "nitter.net"
MINIMAX_API_URL = "https://api.minimax.io/anthropic/v1/messages"
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
AUTH_PROFILES_PATH = Path.home() / ".openclaw" / "agents" / "main" / "agent" / "auth-profiles.json"
# REFERENCE_USER å·²ç§»é™¤ï¼ˆv1.1ï¼‰


# â”€â”€ è®¤è¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_api_config() -> tuple:
    """
    åŠ è½½ AI API é…ç½®ï¼Œè¿”å› (api_key, api_url, model_name, backend)
    ä¼˜å…ˆçº§ï¼š
      1. MINIMAX_API_KEY ç¯å¢ƒå˜é‡
      2. OpenClaw auth-profiles.jsonï¼ˆOpenClaw ç”¨æˆ·è‡ªåŠ¨è¯»å–ï¼‰
      3. OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼ˆå…¼å®¹ä»»ä½• OpenAI æ ¼å¼æ¥å£ï¼‰
    """
    import os

    # 1. ç¯å¢ƒå˜é‡ MINIMAX_API_KEY
    mm_key = os.environ.get("MINIMAX_API_KEY")
    if mm_key:
        return mm_key, MINIMAX_API_URL, "MiniMax-M2.5", "minimax"

    # 2. OpenClaw auth-profiles.json
    try:
        with open(AUTH_PROFILES_PATH) as f:
            data = json.load(f)
        profiles = data.get("profiles", {})
        mm = profiles.get("minimax:default", {})
        key = mm.get("key", "")
        if key:
            return key, MINIMAX_API_URL, "MiniMax-M2.5", "minimax"
    except Exception:
        pass

    # 3. OPENAI_API_KEYï¼ˆå…¼å®¹ OpenAI / DeepSeek / ä»»ä½•å…¼å®¹æ¥å£ï¼‰
    openai_key = os.environ.get("OPENAI_API_KEY")
    openai_url = os.environ.get("OPENAI_BASE_URL", OPENAI_API_URL)
    openai_model = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
    if openai_key:
        return openai_key, openai_url, openai_model, "openai"

    raise RuntimeError(
        "æœªæ‰¾åˆ° AI API Keyã€‚è¯·è®¾ç½®ä»¥ä¸‹ä»»ä¸€ç¯å¢ƒå˜é‡ï¼š\n"
        "  export MINIMAX_API_KEY=your_key   # MiniMaxï¼ˆæ¨èï¼Œå…è´¹é¢åº¦å¤šï¼‰\n"
        "  export OPENAI_API_KEY=your_key    # OpenAI / DeepSeek / å…¼å®¹æ¥å£\n"
        "  export OPENAI_BASE_URL=...        # è‡ªå®šä¹‰æ¥å£åœ°å€ï¼ˆå¯é€‰ï¼‰\n"
        "  export OPENAI_MODEL=gpt-4o-mini   # æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰\n"
        "MiniMax å…è´¹æ³¨å†Œï¼šhttps://www.minimaxi.com"
    )

def load_minimax_key() -> str:
    """å…¼å®¹æ—§ç‰ˆè°ƒç”¨"""
    key, _, _, _ = load_api_config()
    return key


# â”€â”€ æ¨æ–‡æŠ“å– (Camofox + Nitter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_cursor(snapshot: str, username: str) -> Optional[str]:
    """ä»å¿«ç…§ä¸­æå–ä¸‹ä¸€é¡µ cursor"""
    import re
    cursors = re.findall(r'cursor=([^\"&\s\)]+)', snapshot)
    return cursors[0] if cursors else None

def fetch_user_timeline(username: str, count: int = 20, verbose: bool = False) -> Tuple[List[Dict], Dict]:
    """
    é€šè¿‡ Camofox + Nitter æŠ“å–ç”¨æˆ·æ—¶é—´çº¿æ¨æ–‡ï¼ˆæ”¯æŒç¿»é¡µï¼‰
    è¿”å› (tweets_list, user_info)
    """
    MAX_PAGES = 30
    all_tweets: List[Dict] = []
    user_info: Dict = {}
    cursor: Optional[str] = None

    for page in range(1, MAX_PAGES + 1):
        if cursor:
            import urllib.parse
            nitter_url = f"https://{NITTER_INSTANCE}/{username}?cursor={urllib.parse.quote(cursor, safe='')}"
        else:
            nitter_url = f"https://{NITTER_INSTANCE}/{username}"

        if verbose:
            print(f"[Fetcher] ç¬¬{page}é¡µ: {nitter_url}", file=sys.stderr)

        tab_id = _camofox_open_tab(username, nitter_url)
        if not tab_id:
            print(f"[Fetcher] ç¬¬{page}é¡µ Tab åˆ›å»ºå¤±è´¥ï¼Œåœæ­¢", file=sys.stderr)
            break

        time.sleep(8)
        snapshot = _camofox_get_snapshot(tab_id)
        _camofox_close_tab(tab_id)

        if not snapshot:
            print(f"[Fetcher] ç¬¬{page}é¡µå¿«ç…§ä¸ºç©ºï¼Œåœæ­¢", file=sys.stderr)
            break

        # ç¬¬ä¸€é¡µè§£æç”¨æˆ·ä¿¡æ¯
        if page == 1:
            user_info = _parse_user_info(snapshot, username)

        page_tweets = _parse_tweets(snapshot, username, count)
        if not page_tweets:
            if verbose:
                print(f"[Fetcher] ç¬¬{page}é¡µæ— æ¨æ–‡ï¼Œåœæ­¢ç¿»é¡µ", file=sys.stderr)
            break

        all_tweets.extend(page_tweets)
        if verbose:
            print(f"[Fetcher] ç¬¬{page}é¡µæŠ“åˆ° {len(page_tweets)} æ¡ï¼Œç´¯è®¡ {len(all_tweets)} æ¡", file=sys.stderr)

        if len(all_tweets) >= count:
            break

        cursor = _extract_cursor(snapshot, username)
        if not cursor:
            if verbose:
                print(f"[Fetcher] æ— ä¸‹ä¸€é¡µ cursorï¼Œåœæ­¢", file=sys.stderr)
            break

    all_tweets = all_tweets[:count]
    if verbose:
        print(f"[Fetcher] æœ€ç»ˆå…± {len(all_tweets)} æ¡æ¨æ–‡", file=sys.stderr)

    return all_tweets, user_info


def _camofox_open_tab(username: str, url: str) -> Optional[str]:
    """åœ¨ Camofox ä¸­æ‰“å¼€æ–° Tabï¼Œè¿”å› tab_id"""
    try:
        create_data = json.dumps({
            "userId": "x-profile-analyzer",
            "sessionKey": f"profile-{username}-{int(time.time())}",
            "url": url,
        }).encode()

        req = urllib.request.Request(
            f"http://localhost:{CAMOFOX_PORT}/tabs",
            data=create_data,
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        with urllib.request.urlopen(req, timeout=10) as resp:
            tab_data = json.loads(resp.read().decode())

        return tab_data.get("tabId")
    except Exception as e:
        print(f"[Camofox] Error opening tab: {e}", file=sys.stderr)
        return None


def _camofox_get_snapshot(tab_id: str, user_id: str = "x-profile-analyzer") -> str:
    """è·å– Tab å¿«ç…§ï¼ˆuserId å¿…é¡»ä¸åˆ›å»ºæ—¶ä¸€è‡´ï¼‰"""
    try:
        snap_url = f"http://localhost:{CAMOFOX_PORT}/tabs/{tab_id}/snapshot?userId={user_id}"
        req = urllib.request.Request(snap_url)
        with urllib.request.urlopen(req, timeout=15) as resp:
            raw = resp.read().decode("utf-8", errors="replace")
            snap_data = json.loads(raw)
        return snap_data.get("snapshot", "")
    except Exception as e:
        print(f"[Camofox] Error getting snapshot: {e}", file=sys.stderr)
        return ""


def _camofox_close_tab(tab_id: str):
    """å…³é—­ Tab"""
    try:
        close_req = urllib.request.Request(
            f"http://localhost:{CAMOFOX_PORT}/tabs/{tab_id}",
            method="DELETE",
        )
        urllib.request.urlopen(close_req, timeout=5)
    except Exception:
        pass


def _parse_user_info(snapshot: str, username: str) -> Dict:
    """ä»å¿«ç…§ä¸­è§£æç”¨æˆ·åŸºæœ¬ä¿¡æ¯"""
    info = {
        "username": username,
        "display_name": "",
        "bio": "",
        "joined": "",
        "tweets_count": 0,
        "followers": 0,
        "following": 0,
    }

    lines = snapshot.split("\n")
    for i, line in enumerate(lines):
        line = line.strip()

        # æ˜¾ç¤ºåç§°
        if not info["display_name"]:
            m = re.search(r'link\s+"([^@"][^"]+)"\s+\[e\d+\]:', line)
            if m and username.lower() not in m.group(1).lower():
                name = m.group(1)
                if name not in ("nitter", "Logo"):
                    info["display_name"] = name

        # Bio
        if line.startswith("- paragraph:") and not info["bio"]:
            bio = line.replace("- paragraph:", "").strip()
            if bio and "Joined" not in bio:
                info["bio"] = bio

        # Joined
        if "Joined" in line and not info["joined"]:
            m = re.search(r"Joined\s+(.+)", line)
            if m:
                info["joined"] = m.group(1).strip()

        # Stats
        if "Tweets " in line:
            m = re.search(r"Tweets\s+([\d,]+)", line)
            if m:
                info["tweets_count"] = int(m.group(1).replace(",", ""))
        if "Followers " in line:
            m = re.search(r"Followers\s+([\d,]+)", line)
            if m:
                info["followers"] = int(m.group(1).replace(",", ""))
        if "Following " in line:
            m = re.search(r"Following\s+([\d,]+)", line)
            if m:
                info["following"] = int(m.group(1).replace(",", ""))

    return info


def _parse_tweets(snapshot: str, username: str, max_count: int) -> List[Dict]:
    """ä»å¿«ç…§è§£ææ¨æ–‡åˆ—è¡¨"""
    tweets = []
    lines = snapshot.split("\n")

    i = 0
    while i < len(lines) and len(tweets) < max_count:
        line = lines[i].strip()

        # æ£€æµ‹æ¨æ–‡å¼€å¤´: æ—¶é—´é“¾æ¥ (å¦‚ "27m", "9h", "3d")
        time_m = re.search(r'link\s+"(\d+[smhd]|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d+(?:,\s+\d{4})?)"\s+\[e\d+\]:', line)
        if not time_m:
            i += 1
            continue

        # ç¡®è®¤è¿™æ˜¯è¯¥ç”¨æˆ·çš„æ¨æ–‡ (å‰å‡ è¡Œåº”æœ‰ @username)
        is_own_tweet = False
        for j in range(max(0, i - 5), i):
            if f'@{username.lower()}' in lines[j].lower() or f'/{username.lower()}' in lines[j].lower():
                is_own_tweet = True
                break

        if not is_own_tweet:
            i += 1
            continue

        time_str = time_m.group(1)
        tweet_url_m = re.search(r'/url:\s*(/\w+/status/\d+)', lines[i])
        tweet_url = tweet_url_m.group(1) if tweet_url_m else ""

        # æ”¶é›†æ¨æ–‡æ–‡æœ¬ï¼ˆæ¥ä¸‹æ¥çš„æ–‡æœ¬è¡Œï¼‰
        tweet_text_parts = []
        stats_str = ""
        media_urls = []
        quoted_text = ""

        j = i + 1
        while j < min(i + 30, len(lines)):
            next_line = lines[j].strip()

            # ä¸‹ä¸€æ¡æ¨æ–‡å¼€å§‹ï¼ˆæ–°çš„æ—¶é—´é“¾æ¥ï¼‰
            if re.search(r'link\s+"(\d+[smhd]|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d+)"\s+\[e\d+\]:', next_line):
                break

            # æ¨æ–‡æ–‡æœ¬
            if next_line.startswith("- text:") and not next_line.startswith("- text:  "):
                text = next_line.replace("- text:", "").strip()
                # è·³è¿‡ç»Ÿè®¡è¡Œï¼ˆçº¯æ•°å­—+ç©ºæ ¼ï¼‰
                if re.match(r'^[\d\s]+$', text):
                    stats_str = text
                elif text and text not in ("Replying to", "Pinned Tweet"):
                    tweet_text_parts.append(text)

            # åª’ä½“é“¾æ¥
            if "- /url: /pic/" in next_line:
                media_urls.append(next_line.strip())

            # å¼•ç”¨æ¨æ–‡æ–‡æœ¬
            if "- paragraph:" in next_line and j > i + 3:
                quoted_text = next_line.replace("- paragraph:", "").strip()

            j += 1

        tweet_text = " ".join(tweet_text_parts).strip()

        # è§£æäº’åŠ¨æ•°æ®ï¼ˆä» stats_str æå–æ•°å­—ï¼‰
        stats_nums = [int(x) for x in re.findall(r'\d+', stats_str)] if stats_str else []
        replies_count = stats_nums[0] if len(stats_nums) > 0 else 0
        retweets = stats_nums[1] if len(stats_nums) > 1 else 0
        views = stats_nums[2] if len(stats_nums) > 2 else 0

        if tweet_text:  # åªä¿ç•™æœ‰æ–‡æœ¬çš„æ¨æ–‡
            tweet = {
                "text": tweet_text,
                "time": time_str,
                "url": f"https://x.com{tweet_url}" if tweet_url else "",
                "replies": replies_count,
                "retweets": retweets,
                "views": views,
                "has_media": len(media_urls) > 0,
                "quoted_text": quoted_text,
            }
            tweets.append(tweet)

        i = j

    return tweets


# â”€â”€ MiniMax M2.5 åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_profile_with_minimax(
    user_info: Dict,
    tweets: List[Dict],
    api_key: str,
    verbose: bool = False,
    api_url: str = None,
    model_name: str = "MiniMax-M2.5",
    backend: str = "minimax",
) -> str:
    """è°ƒç”¨ AI API ç”Ÿæˆç”¨æˆ·ç”»åƒåˆ†æï¼ˆæ”¯æŒ MiniMax / OpenAI å…¼å®¹æ¥å£ï¼‰"""
    if api_url is None:
        api_url = MINIMAX_API_URL

    # æ„å»ºæ¨æ–‡æ‘˜è¦
    tweets_summary = _build_tweets_summary(tweets)
    user_summary = _build_user_summary(user_info)

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“ç”¨æˆ·åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ @{user_info['username']} çš„æ¨æ–‡æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„ç”¨æˆ·ç”»åƒåˆ†ææŠ¥å‘Šã€‚

## ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
{user_summary}

## æœ€è¿‘æ¨æ–‡ï¼ˆå…± {len(tweets)} æ¡ï¼‰
{tweets_summary}

## åˆ†æè¦æ±‚
è¯·è¾“å‡ºç»“æ„åŒ–çš„ Markdown æ ¼å¼æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

1. **è¯é¢˜åå¥½** - è¯¥ç”¨æˆ·æœ€å¸¸è®¨è®ºçš„ä¸»é¢˜ã€å…³æ³¨é¢†åŸŸã€å…´è¶£æ–¹å‘ï¼Œç»™å‡ºå…·ä½“ä¾‹å­
2. **å†™ä½œé£æ ¼** - è¡¨è¾¾æ–¹å¼ã€è¯­è¨€ä¹ æƒ¯ã€å¥å¼ç‰¹ç‚¹ã€è¡¨æƒ…ç¬¦å·ä½¿ç”¨ï¼Œå¼•ç”¨å®é™…æ¨æ–‡åŸæ–‡ä¸¾ä¾‹
3. **äº’åŠ¨ä¹ æƒ¯** - å‘æ¨é¢‘ç‡ã€å›å¤ä¹ æƒ¯ã€è½¬å‘è¡Œä¸ºï¼Œåˆ†æå…¶ç¤¾äº¤å®šä½ï¼ˆå¹¿æ’­å‹/äº’åŠ¨å‹/æ½œæ°´å‹ï¼‰
4. **æŠ€æœ¯æ–¹å‘** - æ¶‰åŠçš„æŠ€æœ¯æ ˆã€å·¥å…·ã€é¡¹ç›®ã€æŠ€æœ¯è§‚ç‚¹ï¼ˆå¦‚æ— æ˜æ˜¾æŠ€æœ¯å†…å®¹åˆ™æ ‡æ³¨ï¼‰
5. **æ·±å±‚åŠ¨æœºåˆ†æ** - åŸºäºæ¨æ–‡å†…å®¹æ¨æ–­ï¼šè¿™ä¸ªäººå‘æ¨çš„æ ¸å¿ƒé©±åŠ¨åŠ›æ˜¯ä»€ä¹ˆï¼Ÿä»–/å¥¹åœ¨è¿½æ±‚ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆæ½œåœ¨çš„ç„¦è™‘æˆ–æ‰§å¿µï¼Ÿè¿™æ˜¯æŠ¥å‘Šçš„æ ¸å¿ƒç« èŠ‚ï¼Œè¦æœ‰æ´å¯ŸåŠ›ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
6. **è¡Œä¸ºé¢„æµ‹** - åŸºäºå†å²æ¨æ–‡ï¼Œé¢„æµ‹è¿™ä¸ªäººæ¥ä¸‹æ¥æœ€å¯èƒ½åšä»€ä¹ˆï¼Œä¼šå…³æ³¨å“ªäº›è¯é¢˜ï¼Œå¯èƒ½çš„è½¬å˜æ–¹å‘
7. **AI æµ‹ç®—æ˜Ÿåº§** - æ ¹æ®æ¨æ–‡é£æ ¼ã€è¡¨è¾¾ä¹ æƒ¯ã€å…³æ³¨è¯é¢˜ï¼Œç”¨å æ˜Ÿå­¦è§†è§’ç»™å‡º"æœ€åƒå“ªä¸ªæ˜Ÿåº§"ï¼Œé™„ä¸Š 2-3 å¥æœ‰è¶£ç†ç”±ï¼ˆå¨±ä¹å‘ï¼‰
8. **è”ç³»åˆ‡å…¥ç‚¹** - å¦‚æœä½ æƒ³æ¥è§¦è¿™ä¸ªäººï¼Œæœ€å¥½çš„æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿä»–/å¥¹æœ€å®¹æ˜“è¢«ä»€ä¹ˆè¯é¢˜å¸å¼•ï¼Ÿä»€ä¹ˆå¼€åœºç™½ä¼šè®©ä»–/å¥¹æ„¿æ„å›å¤ï¼Ÿç»™å‡º 2-3 ä¸ªå…·ä½“å»ºè®®ï¼Œè¦å®ç”¨ï¼Œä¸è¦å¥—è¯
9. **ä¸€å¥è¯äººç‰©é€Ÿå†™** - ç”¨ä¸€å¥è¯ç²¾å‡†æ¦‚æ‹¬è¿™ä¸ªäººï¼Œè¦æœ‰è®°å¿†ç‚¹ï¼Œåƒä¸€ä¸ªå¥½çš„äººç‰©ä¼ è®°å¼€å¤´

è¯·ä¿æŒåˆ†ææ·±åˆ»ã€å…·ä½“ã€æœ‰æ´å¯ŸåŠ›ï¼ŒåŸºäºå®é™…æ¨æ–‡å†…å®¹ï¼Œé¿å…å¥—è¯ã€‚"""

    if verbose:
        print(f"[MiniMax] Sending {len(tweets)} tweets for analysis...", file=sys.stderr)

    try:
        request_body = json.dumps({
            "model": "MiniMax-M1",
            "max_tokens": 4096,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
        }).encode("utf-8")

        req = urllib.request.Request(
            MINIMAX_API_URL,
            data=request_body,
            headers={
                "Content-Type": "application/json",
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01",
            },
            method="POST",
        )

        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read().decode("utf-8"))

        # æå–æ–‡æœ¬
        content = result.get("content", [])
        for block in content:
            if block.get("type") == "text":
                return block["text"]

        return f"[Error] Unexpected API response format: {json.dumps(result)[:500]}"

    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", errors="replace")[:500]
        raise RuntimeError(f"MiniMax API HTTP {e.code}: {body}")
    except urllib.error.URLError as e:
        raise RuntimeError(f"MiniMax API connection error: {e.reason}")
    except TimeoutError:
        raise RuntimeError("MiniMax API request timed out (>120s). Try reducing --count.")


def _build_user_summary(user_info: Dict) -> str:
    lines = [
        f"- ç”¨æˆ·å: @{user_info.get('username', 'unknown')}",
        f"- æ˜¾ç¤ºåç§°: {user_info.get('display_name', 'N/A')}",
        f"- ç®€ä»‹: {user_info.get('bio', 'N/A')}",
        f"- åŠ å…¥æ—¶é—´: {user_info.get('joined', 'N/A')}",
        f"- æ¨æ–‡æ•°: {user_info.get('tweets_count', 0):,}",
        f"- ç²‰ä¸æ•°: {user_info.get('followers', 0):,}",
        f"- å…³æ³¨æ•°: {user_info.get('following', 0):,}",
    ]
    return "\n".join(lines)


def _parse_tweet_date(time_str: str) -> Optional[datetime]:
    """æŠŠ Nitter æ—¶é—´å­—ç¬¦ä¸²è§£ææˆ datetimeï¼ˆå°½åŠ›è€Œä¸ºï¼‰"""
    from datetime import timedelta
    now = datetime.now()
    if not time_str:
        return None
    # ç›¸å¯¹æ—¶é—´ï¼š2h / 15m / 3d / 5s
    m = re.match(r'^(\d+)([smhd])$', time_str.strip())
    if m:
        n, unit = int(m.group(1)), m.group(2)
        delta = {'s': timedelta(seconds=n), 'm': timedelta(minutes=n),
                 'h': timedelta(hours=n), 'd': timedelta(days=n)}[unit]
        return now - delta
    # ç»å¯¹æ—¶é—´ï¼šJan 19 æˆ– Jan 19, 2026
    for fmt in ("%b %d, %Y", "%b %d"):
        try:
            dt = datetime.strptime(time_str.strip(), fmt)
            if fmt == "%b %d":
                dt = dt.replace(year=now.year)
                # å¦‚æœè§£æå‡ºæ¥æ˜¯æœªæ¥æ—¥æœŸï¼Œè¯´æ˜æ˜¯å»å¹´
                if dt > now:
                    dt = dt.replace(year=now.year - 1)
            return dt
        except ValueError:
            continue
    return None


def _build_activity_heatmap(tweets: List[Dict]) -> str:
    """ç”Ÿæˆæ¨æ–‡æ˜ŸæœŸåˆ†å¸ƒ ASCII çƒ­åŠ›å›¾"""
    from collections import Counter
    weekday_names = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
    weekday_cn = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"]

    counts = Counter()
    parsed = 0
    for t in tweets:
        dt = _parse_tweet_date(t.get("time", ""))
        if dt:
            counts[dt.weekday()] += 1
            parsed += 1

    if parsed < 10:
        return ""  # æ•°æ®å¤ªå°‘ï¼Œä¸ç”Ÿæˆ

    total = sum(counts.values())
    max_count = max(counts.values()) if counts else 1
    bar_width = 20

    lines = [f"\n## æ´»è·ƒæ—¶é—´åˆ†æ\n", f"å‘æ¨æ˜ŸæœŸåˆ†å¸ƒï¼ˆå…± {parsed} æ¡æœ‰æ•ˆæ•°æ®ï¼‰ï¼š\n"]
    for i, (name, cn) in enumerate(zip(weekday_names, weekday_cn)):
        c = counts.get(i, 0)
        pct = c / total * 100 if total else 0
        filled = int(c / max_count * bar_width)
        bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
        lines.append(f"{name} {bar} {c:3d} æ¡ ({pct:.0f}%)")

    # æœ€æ´»è·ƒ / æœ€æ²‰é»˜
    if counts:
        peak_day = max(counts, key=counts.get)
        quiet_day = min(counts, key=counts.get)
        lines.append(f"\nğŸ”¥ æœ€æ´»è·ƒï¼š{weekday_cn[peak_day]}  ğŸ“‰ æœ€æ²‰é»˜ï¼š{weekday_cn[quiet_day]}")

        # å·¥ä½œæ—¥ vs å‘¨æœ«
        workday = sum(counts.get(i, 0) for i in range(5))
        weekend = sum(counts.get(i, 0) for i in range(5, 7))
        if total > 0:
            if workday / total > 0.7:
                lines.append("ğŸ’¡ å·¥ä½œæ—¥é©±åŠ¨å‹ï¼Œå‘¨æœ«æ˜æ˜¾å‡å°‘")
            elif weekend / total > 0.4:
                lines.append("ğŸ’¡ å‘¨æœ«æ´»è·ƒå‹ï¼Œå·¥ä½œæ—¥è¾“å‡ºå°‘")
            else:
                lines.append("ğŸ’¡ å…¨å‘¨å‡è¡¡è¾“å‡ºï¼Œæ— æ˜æ˜¾è§„å¾‹")

    return "\n".join(lines)


def _build_tweets_summary(tweets: List[Dict]) -> str:
    parts = []
    for i, t in enumerate(tweets, 1):
        text = t["text"]
        stats = f"å›å¤:{t['replies']} è½¬æ¨:{t['retweets']} æµè§ˆ:{t['views']}"
        has_media = "ğŸ“·" if t.get("has_media") else ""
        quoted = f"\n  > å¼•ç”¨: {t['quoted_text'][:100]}" if t.get("quoted_text") else ""
        parts.append(f"{i}. [{t['time']}] {has_media}{text[:300]}{quoted}\n   ({stats})")
    return "\n\n".join(parts)


# â”€â”€ è¾“å‡ºæ ¼å¼åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_report(user_info: Dict, tweets: List[Dict], analysis: str) -> str:
    """ç”Ÿæˆæœ€ç»ˆ Markdown æŠ¥å‘Š"""
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    username = user_info.get("username", "unknown")
    display_name = user_info.get("display_name", username)
    tweet_count = len(tweets)

    # æ•°æ®è´¨é‡æ ‡æ³¨
    if tweet_count < 50:
        data_quality = f"âš ï¸ ä½ï¼ˆä»… {tweet_count} æ¡ï¼ŒNitter å¯¹è¯¥è´¦å·æ”¶å½•ä¸è¶³ï¼Œç»“æœä»…ä¾›å‚è€ƒï¼‰"
    elif tweet_count < 100:
        data_quality = f"âš¡ ä¸­ï¼ˆ{tweet_count} æ¡ï¼Œå»ºè®® 100+ æ¡è·å¾—æ›´å‡†ç¡®åˆ†æï¼‰"
    else:
        data_quality = f"âœ… é«˜ï¼ˆ{tweet_count} æ¡ï¼‰"

    header = f"""# ç”¨æˆ·ç”»åƒåˆ†ææŠ¥å‘Šï¼š@{username}

> ç”Ÿæˆæ—¶é—´ï¼š{now}
> åˆ†æå·¥å…·ï¼šx-profile-analyzer v1.2
> æ•°æ®æ¥æºï¼šNitter / X.com
> æ•°æ®è´¨é‡ï¼š{data_quality}

## åŸºæœ¬ä¿¡æ¯

| å­—æ®µ | å€¼ |
|------|-----|
| ç”¨æˆ·å | @{username} |
| æ˜¾ç¤ºåç§° | {display_name} |
| ç®€ä»‹ | {user_info.get('bio', 'N/A')} |
| åŠ å…¥æ—¶é—´ | {user_info.get('joined', 'N/A')} |
| æ¨æ–‡æ•° | {user_info.get('tweets_count', 0):,} |
| ç²‰ä¸æ•° | {user_info.get('followers', 0):,} |
| å…³æ³¨æ•° | {user_info.get('following', 0):,} |

*æœ¬æ¬¡åˆ†æåŸºäºæœ€è¿‘ {len(tweets)} æ¡æ¨æ–‡*

---

"""

    heatmap = _build_activity_heatmap(tweets)
    return header + analysis + heatmap + f"\n\n---\n*åˆ†æç”± AI ç”Ÿæˆ | x-profile-analyzer v1.5*\n"


# â”€â”€ ä¸»ç¨‹åº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="X ç”¨æˆ·ç”»åƒåˆ†æå·¥å…· - æŠ“å–æ¨æ–‡ï¼Œå¯é€‰ AI åˆ†æ"
    )
    parser.add_argument("--user", "-u", required=True, help="X/Twitter ç”¨æˆ·åï¼ˆä¸å« @ï¼‰")
    parser.add_argument("--count", "-c", type=int, default=300, help="æŠ“å–æ¨æ–‡æ•°é‡ï¼ˆé»˜è®¤ 300ï¼ŒNitter å®é™…ä¸Šé™çº¦ 300ï¼‰")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è¾“å‡ºåˆ° stdoutï¼‰")
    parser.add_argument("--output-json", help="åŒæ—¶ä¿å­˜åŸå§‹æ¨æ–‡ JSON åˆ°æŒ‡å®šè·¯å¾„")
    parser.add_argument("--no-analyze", action="store_true", help="åªæŠ“æ¨æ–‡æ•°æ®ï¼Œä¸è°ƒ AI åˆ†æï¼ˆè®©è°ƒç”¨æ–¹è‡ªå·±åˆ†æï¼‰")
    parser.add_argument("--verbose", "-v", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¿›åº¦ä¿¡æ¯")
    parser.add_argument("--no-camofox", action="store_true", help="è·³è¿‡ Camofox æ£€æŸ¥ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    args = parser.parse_args()

    username = args.user.lstrip("@")

    # æ£€æŸ¥ Camofox çŠ¶æ€
    if not args.no_camofox:
        try:
            req = urllib.request.Request(f"http://localhost:{CAMOFOX_PORT}/")
            with urllib.request.urlopen(req, timeout=3) as resp:
                status = json.loads(resp.read().decode())
            if not status.get("running"):
                print(f"[Error] Camofox is not running. Start it first.", file=sys.stderr)
                sys.exit(1)
            if args.verbose:
                print(f"[Camofox] Status: OK (browser connected: {status.get('browserConnected')})", file=sys.stderr)
        except Exception as e:
            print(f"[Error] Cannot connect to Camofox at port {CAMOFOX_PORT}: {e}", file=sys.stderr)
            print("Make sure Camofox is running.", file=sys.stderr)
            sys.exit(1)

    # æŠ“å–æ¨æ–‡
    print(f"ğŸ“Š æ­£åœ¨æŠ“å– @{username} çš„æ¨æ–‡...", file=sys.stderr)
    try:
        tweets, user_info = fetch_user_timeline(username, args.count, verbose=args.verbose)
    except RuntimeError as e:
        print(f"[Error] Failed to fetch tweets: {e}", file=sys.stderr)
        sys.exit(1)

    if not tweets:
        print(f"[Warning] No tweets found for @{username}. Account may be protected or not exist.", file=sys.stderr)
        sys.exit(1)

    print(f"âœ… æˆåŠŸè·å– {len(tweets)} æ¡æ¨æ–‡", file=sys.stderr)

    # æ•°æ®è´¨é‡æç¤º
    if len(tweets) < 50:
        print(f"âš ï¸  æ•°æ®ä¸è¶³ï¼ˆä»… {len(tweets)} æ¡ï¼‰ï¼šè¯¥è´¦å·åœ¨ Nitter æ”¶å½•è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯å°è´¦å·æˆ–ä½æ´»è·ƒåº¦è´¦å·ï¼Œåˆ†æç»“æœä»…ä¾›å‚è€ƒ", file=sys.stderr)
    elif len(tweets) < 100:
        print(f"âš ï¸  æ•°æ®åå°‘ï¼ˆ{len(tweets)} æ¡ï¼‰ï¼šå»ºè®® 100 æ¡ä»¥ä¸Šä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æ", file=sys.stderr)

    # ä¿å­˜åŸå§‹ JSONï¼ˆå¯é€‰ï¼‰
    if args.output_json:
        json_path = Path(args.output_json)
        json_path.parent.mkdir(parents=True, exist_ok=True)
        json_path.write_text(json.dumps({
            "user_info": user_info,
            "tweets": tweets,
            "fetched_at": time.strftime("%Y-%m-%d %H:%M"),
            "tweet_count": len(tweets),
        }, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"âœ… åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {json_path}", file=sys.stderr)

    # --no-analyzeï¼šåªè¾“å‡ºç»“æ„åŒ–æ•°æ®ï¼Œè®©è°ƒç”¨æ–¹è‡ªå·±åˆ†æ
    if args.no_analyze:
        output = _build_data_summary(user_info, tweets)
        if args.output:
            Path(args.output).write_text(output, encoding="utf-8")
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {args.output}", file=sys.stderr)
        else:
            print(output)
        return

    # AI åˆ†ææ¨¡å¼ï¼šåŠ è½½ API Key
    try:
        api_key, api_url, model_name, backend = load_api_config()
        if args.verbose:
            print(f"[Auth] {backend} API loaded: {api_key[:15]}... model={model_name}", file=sys.stderr)
    except RuntimeError as e:
        print(f"[Error] {e}", file=sys.stderr)
        print("æç¤ºï¼šä½¿ç”¨ --no-analyze å¯è·³è¿‡ AI åˆ†æï¼Œç›´æ¥è¾“å‡ºæ¨æ–‡æ•°æ®", file=sys.stderr)
        sys.exit(1)

    print(f"ğŸ¤– æ­£åœ¨åˆ†æç”¨æˆ·ç”»åƒ...", file=sys.stderr)
    try:
        analysis = analyze_profile_with_minimax(user_info, tweets, api_key, verbose=args.verbose,
                                                 api_url=api_url, model_name=model_name, backend=backend)
    except RuntimeError as e:
        print(f"[Error] Analysis failed: {e}", file=sys.stderr)
        sys.exit(1)

    report = format_report(user_info, tweets, analysis)

    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(report, encoding="utf-8")
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}", file=sys.stderr)
    else:
        print(report)


def _build_data_summary(user_info: Dict, tweets: List[Dict]) -> str:
    """--no-analyze æ¨¡å¼ï¼šè¾“å‡ºç»“æ„åŒ–æ¨æ–‡æ•°æ®ï¼Œä¾›è°ƒç”¨æ–¹è‡ªè¡Œåˆ†æ"""
    lines = [
        f"# @{user_info.get('username', 'unknown')} æ¨æ–‡æ•°æ®",
        f"",
        f"## åŸºæœ¬ä¿¡æ¯",
        f"- ç”¨æˆ·å: @{user_info.get('username')}",
        f"- æ˜¾ç¤ºåç§°: {user_info.get('display_name', 'N/A')}",
        f"- ç®€ä»‹: {user_info.get('bio', 'N/A')}",
        f"- åŠ å…¥æ—¶é—´: {user_info.get('joined', 'N/A')}",
        f"- æ¨æ–‡æ•°: {user_info.get('tweets_count', 'N/A')}",
        f"- ç²‰ä¸æ•°: {user_info.get('followers', 'N/A')}",
        f"- å…³æ³¨æ•°: {user_info.get('following', 'N/A')}",
        f"",
        f"## æ¨æ–‡åˆ—è¡¨ï¼ˆå…± {len(tweets)} æ¡ï¼‰",
        f"",
    ]
    for i, t in enumerate(tweets, 1):
        lines.append(f"### [{i}] {t.get('time', '')} | ğŸ’¬{t.get('replies',0)} ğŸ”{t.get('retweets',0)} â¤ï¸{t.get('views',0)}")
        lines.append(t.get('text', '').strip())
        lines.append("")
    # åŠ çƒ­åŠ›å›¾
    heatmap = _build_activity_heatmap(tweets)
    if heatmap:
        lines.append(heatmap)
    lines.append("\n---")
    lines.append(f"*x-profile-analyzer v1.5 | æ•°æ®æŠ“å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M')}*")
    return "\n".join(lines)


if __name__ == "__main__":
    main()
